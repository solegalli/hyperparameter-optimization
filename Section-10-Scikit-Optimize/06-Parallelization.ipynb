{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization with Scikit-Optimize\n",
    "\n",
    "In this notebook, we will perform **Bayesian Optimization** with Gaussian Processes in Parallel, utilizing various CPUs, to speed up the search.\n",
    "\n",
    "This is useful to reduce search times.\n",
    "\n",
    "https://scikit-optimize.readthedocs.io/en/latest/auto_examples/parallel-optimization.html#sphx-glr-auto-examples-parallel-optimization-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "\n",
    "from skopt import Optimizer # for the optimization\n",
    "from joblib import Parallel, delayed # for the parallelization\n",
    "\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1       2       3        4        5       6        7       8   \\\n",
       "0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710  0.2419   \n",
       "1  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017  0.1812   \n",
       "2  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790  0.2069   \n",
       "3  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520  0.2597   \n",
       "4  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430  0.1809   \n",
       "\n",
       "        9   ...     20     21      22      23      24      25      26      27  \\\n",
       "0  0.07871  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.05667  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.05999  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.09744  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.05883  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "\n",
       "       28       29  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "breast_cancer_X, breast_cancer_y = load_breast_cancer(return_X_y=True)\n",
    "X = pd.DataFrame(breast_cancer_X)\n",
    "y = pd.Series(breast_cancer_y).map({0:1, 1:0})\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.627417\n",
       "1    0.372583\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the target:\n",
    "# percentage of benign (0) and malign tumors (1)\n",
    "\n",
    "y.value_counts() / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((398, 30), (171, 30))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split dataset into a train and test set\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Hyperparameter Space\n",
    "\n",
    "Scikit-optimize provides an utility function to create the range of values to examine for each hyperparameters. More details in [skopt.Space](https://scikit-optimize.github.io/stable/modules/generated/skopt.Space.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the hyperparameter space\n",
    "\n",
    "param_grid = [\n",
    "    Integer(10, 120, name=\"n_estimators\"),\n",
    "    Integer(1, 5, name=\"max_depth\"),\n",
    "    Real(0.0001, 0.1, prior='log-uniform', name='learning_rate'),\n",
    "    Real(0.001, 0.999, prior='log-uniform', name=\"min_samples_split\"),\n",
    "    Categorical(['log_loss', 'exponential'], name=\"loss\"),\n",
    "]\n",
    "\n",
    "# Scikit-optimize parameter grid is a list\n",
    "type(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the gradient boosting classifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the objective function\n",
    "\n",
    "This is the hyperparameter response space, the function we want to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We design a function to maximize the accuracy, of a GBM,\n",
    "# with cross-validation\n",
    "\n",
    "# the decorator allows our objective function to receive the parameters as\n",
    "# keyword arguments. This is a requirement for scikit-optimize.\n",
    "\n",
    "@use_named_args(param_grid)\n",
    "def objective(**params):\n",
    "    \n",
    "    # model with new parameters\n",
    "    gbm.set_params(**params)\n",
    "\n",
    "    # optimization function (hyperparam response function)\n",
    "    value = np.mean(\n",
    "        cross_val_score(\n",
    "            gbm, \n",
    "            X_train,\n",
    "            y_train,\n",
    "            cv=3,\n",
    "            n_jobs=-4,\n",
    "            scoring='accuracy')\n",
    "    )\n",
    "\n",
    "    # negate because we need to minimize\n",
    "    return -value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization with Gaussian Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Optimizer\n",
    "\n",
    "optimizer = Optimizer(\n",
    "    dimensions = param_grid, # the hyperparameter space\n",
    "    base_estimator = \"GP\", # the surrogate\n",
    "    n_initial_points=10, # the number of points to evaluate f(x) to start of\n",
    "    acq_func='EI', # the acquisition function\n",
    "    random_state=0, \n",
    "    n_jobs=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sole\\Documents\\Repositories\\envs\\fsml\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:517: UserWarning: The objective has been evaluated at point [120, 1, 0.1, 0.999, 'exponential'] before, using random point [43, 2, 0.032107400739456116, 0.005537305298847769, 'log_loss']\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# we will use 4 CPUs (n_points)\n",
    "# if we loop 10 times using 4 end points, we perform 40 searches in total\n",
    "\n",
    "for i in range(10):\n",
    "    x = optimizer.ask(n_points=4)  # x is a list of n_points points\n",
    "    y = Parallel(n_jobs=4)(delayed(objective)(v) for v in x)  # evaluate points in parallel\n",
    "    optimizer.tell(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[68, 4, 0.007381238832487747, 0.08704800719052391, 'exponential'],\n",
       " [118, 2, 0.00010113718979245275, 0.05725990689319986, 'log_loss'],\n",
       " [17, 3, 0.00022221129238269847, 0.17371729808265857, 'exponential'],\n",
       " [42, 4, 0.000960176974739521, 0.6793527084375192, 'exponential'],\n",
       " [38, 5, 0.053126979002083165, 0.06277193933628669, 'log_loss'],\n",
       " [28, 4, 0.005445788189169609, 0.002258808366805843, 'log_loss'],\n",
       " [56, 1, 0.0006780193306440557, 0.9274466173670369, 'exponential'],\n",
       " [42, 4, 0.08952334707464486, 0.20644568894340298, 'log_loss'],\n",
       " [68, 1, 0.0010637763908757617, 0.0037524397848558923, 'log_loss'],\n",
       " [48, 4, 0.0016815858162959685, 0.27886812907463643, 'log_loss'],\n",
       " [107, 5, 0.016812145780049137, 0.008608554188871567, 'exponential'],\n",
       " [10, 2, 0.05193389864279602, 0.2421867730253962, 'exponential'],\n",
       " [25, 3, 0.07191463533071468, 0.09221435880215857, 'log_loss'],\n",
       " [120, 5, 0.07697017752330598, 0.8551443403588459, 'log_loss'],\n",
       " [10, 2, 0.0784246754868581, 0.0011887833579162818, 'log_loss'],\n",
       " [10, 4, 0.08121611587709748, 0.999, 'log_loss'],\n",
       " [120, 1, 0.0777302892859414, 0.001, 'log_loss'],\n",
       " [120, 1, 0.07607563983863629, 0.001, 'log_loss'],\n",
       " [120, 1, 0.07630178973956034, 0.03496002313057386, 'log_loss'],\n",
       " [114, 5, 0.07804002026969835, 0.001, 'log_loss'],\n",
       " [120, 2, 0.08059098995484927, 0.999, 'log_loss'],\n",
       " [120, 1, 0.08975112853719557, 0.999, 'log_loss'],\n",
       " [120, 1, 0.08183657339038516, 0.999, 'log_loss'],\n",
       " [120, 1, 0.018369652225054815, 0.5738087611911074, 'exponential'],\n",
       " [120, 1, 0.1, 0.999, 'log_loss'],\n",
       " [120, 1, 0.1, 0.2354046740106873, 'exponential'],\n",
       " [120, 5, 0.1, 0.999, 'log_loss'],\n",
       " [120, 5, 0.013238675684027342, 0.2344522128076889, 'exponential'],\n",
       " [10, 1, 0.1, 0.001, 'exponential'],\n",
       " [120, 1, 0.1, 0.999, 'exponential'],\n",
       " [43, 2, 0.032107400739456116, 0.005537305298847769, 'log_loss'],\n",
       " [120, 1, 0.031673597285201145, 0.999, 'log_loss'],\n",
       " [71, 5, 0.019529014779919144, 0.1606944952590533, 'log_loss'],\n",
       " [74, 1, 0.016204153446650656, 0.999, 'log_loss'],\n",
       " [76, 5, 0.014076582128735428, 0.001, 'exponential'],\n",
       " [65, 1, 0.041671629253487004, 0.001, 'exponential'],\n",
       " [120, 5, 0.03425140374547125, 0.999, 'log_loss'],\n",
       " [49, 1, 0.062469080738000206, 0.999, 'log_loss'],\n",
       " [63, 1, 0.1, 0.0023581542901557223, 'log_loss'],\n",
       " [57, 1, 0.0757640363788298, 0.001, 'exponential']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the evaluated hyperparamters\n",
    "\n",
    "optimizer.Xi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.9171413381939697,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9296536796536796,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9347231715652767,\n",
       " -0.6256360598465861,\n",
       " -0.6256360598465861,\n",
       " -0.9221348826611985,\n",
       " -0.9146350725298094,\n",
       " -0.9321979190400244,\n",
       " -0.9673425989215462,\n",
       " -0.9246601351864511,\n",
       " -0.8994645705172021,\n",
       " -0.9497607655502392,\n",
       " -0.9472355130249867,\n",
       " -0.9472355130249867,\n",
       " -0.9221538695222905,\n",
       " -0.949741778689147,\n",
       " -0.9572985494038125,\n",
       " -0.949741778689147,\n",
       " -0.9346662109820004,\n",
       " -0.9572795625427205,\n",
       " -0.9572795625427205,\n",
       " -0.9572795625427205,\n",
       " -0.9297296270980482,\n",
       " -0.9170274170274171,\n",
       " -0.9572795625427205,\n",
       " -0.9296726665147718,\n",
       " -0.9472544998860788,\n",
       " -0.9272043745727956,\n",
       " -0.9170274170274171,\n",
       " -0.9221348826611985,\n",
       " -0.9497607655502392,\n",
       " -0.9497607655502392,\n",
       " -0.9472544998860788,\n",
       " -0.9447102604997343,\n",
       " -0.9447102604997341]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the accuracy\n",
    "\n",
    "optimizer.yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.057260</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.173717</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.679353</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053127</td>\n",
       "      <td>0.062772</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_estimators  max_depth  min_samples_split  learning_rate         loss  \\\n",
       "0            68          4           0.007381       0.087048  exponential   \n",
       "1           118          2           0.000101       0.057260     log_loss   \n",
       "2            17          3           0.000222       0.173717  exponential   \n",
       "3            42          4           0.000960       0.679353  exponential   \n",
       "4            38          5           0.053127       0.062772     log_loss   \n",
       "\n",
       "   accuracy  \n",
       "0 -0.917141  \n",
       "1 -0.625636  \n",
       "2 -0.625636  \n",
       "3 -0.625636  \n",
       "4 -0.929654  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all together in one dataframe, so we can investigate further\n",
    "dim_names = ['n_estimators', 'max_depth', 'min_samples_split', 'learning_rate', 'loss']\n",
    "\n",
    "tmp = pd.concat([\n",
    "    pd.DataFrame(optimizer.Xi),\n",
    "    pd.Series(optimizer.yi),\n",
    "], axis=1)\n",
    "\n",
    "tmp.columns = dim_names + ['accuracy']\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate convergence of the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp['accuracy'].sort_values(ascending=False).reset_index(drop=True).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trade-off with parallelization, is that we will not optimize the search after each evaluation of f(x), instead after, in this case 4, evaluations of f(x). Thus, we may need to perform more evaluations to find the optima. But, because we do it in parallel, overall, we reduce wall time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_estimators</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.076970</td>\n",
       "      <td>0.855144</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.967343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.089751</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.957299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.235405</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.957280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.034251</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.041672</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.077730</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.949761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.081837</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.949742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>120</td>\n",
       "      <td>2</td>\n",
       "      <td>0.080591</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.949742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062469</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.947254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.031674</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.947254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076302</td>\n",
       "      <td>0.034960</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.947236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.076076</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.947236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.944710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0.075764</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.944710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.089523</td>\n",
       "      <td>0.206446</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.934723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>0.573809</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.934666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.071915</td>\n",
       "      <td>0.092214</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.932198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>0.013239</td>\n",
       "      <td>0.234452</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.929730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>0.032107</td>\n",
       "      <td>0.005537</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.929673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053127</td>\n",
       "      <td>0.062772</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.929654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>71</td>\n",
       "      <td>5</td>\n",
       "      <td>0.019529</td>\n",
       "      <td>0.160694</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.927204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.078425</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.924660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>114</td>\n",
       "      <td>5</td>\n",
       "      <td>0.078040</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.922154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>107</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016812</td>\n",
       "      <td>0.008609</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.922135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "      <td>0.014077</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.922135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.917027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016204</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.917027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.051934</td>\n",
       "      <td>0.242187</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.914635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.081216</td>\n",
       "      <td>0.999000</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.899465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.278868</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.927447</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.005446</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.679353</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.173717</td>\n",
       "      <td>exponential</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.057260</td>\n",
       "      <td>log_loss</td>\n",
       "      <td>-0.625636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n_estimators  max_depth  min_samples_split  learning_rate         loss  \\\n",
       "13           120          5           0.076970       0.855144     log_loss   \n",
       "21           120          1           0.089751       0.999000     log_loss   \n",
       "24           120          1           0.100000       0.999000     log_loss   \n",
       "29           120          1           0.100000       0.999000  exponential   \n",
       "26           120          5           0.100000       0.999000     log_loss   \n",
       "25           120          1           0.100000       0.235405  exponential   \n",
       "36           120          5           0.034251       0.999000     log_loss   \n",
       "35            65          1           0.041672       0.001000  exponential   \n",
       "16           120          1           0.077730       0.001000     log_loss   \n",
       "22           120          1           0.081837       0.999000     log_loss   \n",
       "20           120          2           0.080591       0.999000     log_loss   \n",
       "37            49          1           0.062469       0.999000     log_loss   \n",
       "31           120          1           0.031674       0.999000     log_loss   \n",
       "18           120          1           0.076302       0.034960     log_loss   \n",
       "17           120          1           0.076076       0.001000     log_loss   \n",
       "38            63          1           0.100000       0.002358     log_loss   \n",
       "39            57          1           0.075764       0.001000  exponential   \n",
       "7             42          4           0.089523       0.206446     log_loss   \n",
       "23           120          1           0.018370       0.573809  exponential   \n",
       "12            25          3           0.071915       0.092214     log_loss   \n",
       "27           120          5           0.013239       0.234452  exponential   \n",
       "30            43          2           0.032107       0.005537     log_loss   \n",
       "4             38          5           0.053127       0.062772     log_loss   \n",
       "32            71          5           0.019529       0.160694     log_loss   \n",
       "14            10          2           0.078425       0.001189     log_loss   \n",
       "19           114          5           0.078040       0.001000     log_loss   \n",
       "10           107          5           0.016812       0.008609  exponential   \n",
       "34            76          5           0.014077       0.001000  exponential   \n",
       "0             68          4           0.007381       0.087048  exponential   \n",
       "28            10          1           0.100000       0.001000  exponential   \n",
       "33            74          1           0.016204       0.999000     log_loss   \n",
       "11            10          2           0.051934       0.242187  exponential   \n",
       "15            10          4           0.081216       0.999000     log_loss   \n",
       "9             48          4           0.001682       0.278868     log_loss   \n",
       "8             68          1           0.001064       0.003752     log_loss   \n",
       "6             56          1           0.000678       0.927447  exponential   \n",
       "5             28          4           0.005446       0.002259     log_loss   \n",
       "3             42          4           0.000960       0.679353  exponential   \n",
       "2             17          3           0.000222       0.173717  exponential   \n",
       "1            118          2           0.000101       0.057260     log_loss   \n",
       "\n",
       "    accuracy  \n",
       "13 -0.967343  \n",
       "21 -0.957299  \n",
       "24 -0.957280  \n",
       "29 -0.957280  \n",
       "26 -0.957280  \n",
       "25 -0.957280  \n",
       "36 -0.949761  \n",
       "35 -0.949761  \n",
       "16 -0.949761  \n",
       "22 -0.949742  \n",
       "20 -0.949742  \n",
       "37 -0.947254  \n",
       "31 -0.947254  \n",
       "18 -0.947236  \n",
       "17 -0.947236  \n",
       "38 -0.944710  \n",
       "39 -0.944710  \n",
       "7  -0.934723  \n",
       "23 -0.934666  \n",
       "12 -0.932198  \n",
       "27 -0.929730  \n",
       "30 -0.929673  \n",
       "4  -0.929654  \n",
       "32 -0.927204  \n",
       "14 -0.924660  \n",
       "19 -0.922154  \n",
       "10 -0.922135  \n",
       "34 -0.922135  \n",
       "0  -0.917141  \n",
       "28 -0.917027  \n",
       "33 -0.917027  \n",
       "11 -0.914635  \n",
       "15 -0.899465  \n",
       "9  -0.625636  \n",
       "8  -0.625636  \n",
       "6  -0.625636  \n",
       "5  -0.625636  \n",
       "3  -0.625636  \n",
       "2  -0.625636  \n",
       "1  -0.625636  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.sort_values(by='accuracy', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsml",
   "language": "python",
   "name": "fsml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
